{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter vs. Public"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents\n",
    "- Introduction\n",
    "- Data Collection\n",
    "- Data Cleaning\n",
    "- Analysis\n",
    "- Results\n",
    "- Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"Introduction\"></a>\n",
    "\n",
    "This work seeks to provide evidence on the differences in sentiment (Positive/Negative) between perceptions shared on twitter versus perceptions held by the public at large. Specifically, this analysis will be limited to sentiment of famous brands between the two groups. The public's sentiment is approximated by brand specific survey data collected by YouGov. Whereas for Twitter data, tweets responding to these brands will be scraped and classified as either positive or negative. \n",
    "\n",
    "It is worth noting that the publicly available YouGov brand data is country specific. To ensure the validity of the data, only UK data will be considered. The UK was chosen for three reasons. First, in most countries, large brands have region specific twitter accounts (i.e., Pizza Hut has the @pizzahutuk twitter account). However, often times for the United States, brands opt not to have a US specific account (i.e., the American Pizza Hut account is simply @pizzahut). It is fair to assume that someone responding to a brand's regional account has a high likelihood of being from that region. However, it is not fair to assume that tweets directed at a brand's main account are from the US. Second, the UK is a predominantly English speaking and writing country, enabling me to process the text of the replies. Third, I could not find this same level of data publicly available for any other country satisfying the above two criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules used for analysis, version numbers are available in packages.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import dill\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection <a name=\"DataCollection\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the survey data and Twitter data were scraped using the Selenium library. The benefit of Selenium for this project comes from the ability to click on buttons, collect data not in a tabular form, and navigate to links within other webpages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survey Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data comes from https://yougov.co.uk/ratings/consumer/fame/brands/all (collected 2022 Q2 data). Only brands with fame of at least 95% were included. This criterion was chosen with the assumption that more famous brands are more likely to have regional twitter accounts, and with a greater number of replies than their less famous counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Downloading: 100%|██████████| 6.21M/6.21M [00:00<00:00, 7.57MB/s]\n",
      "c:\\Users\\franc\\Documents\\learning\\Projects\\public_vs_twitter\\public_vs_twitter_env\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  if sys.path[0] == \"\":\n"
     ]
    }
   ],
   "source": [
    "public_data_url = \"https://yougov.co.uk/ratings/consumer/fame/brands/all\" #survey data site\n",
    "\n",
    "def page_down_wait_one_second(webdriver, body='/html/body'):\n",
    "    webdriver.find_element(By.XPATH, body).send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "\n",
    "def repeat_page_down(number_of_times, webdriver, body='/html/body'):\n",
    "    for x in range(0,number_of_times):\n",
    "        page_down_wait_one_second(webdriver, body='/html/body')\n",
    "\n",
    "# chunk below: going to survey url and getting all data on screen\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(public_data_url)\n",
    "driver.find_element(By.ID, \"onetrust-accept-btn-handler\").send_keys(Keys.ENTER)\n",
    "driver.find_element(By.NAME, \"rankings-load-more-entities\").send_keys(Keys.ENTER)\n",
    "repeat_page_down(70, driver) # after viewing the webpage, pressing page down 70 times is sufficient to capture all relevant data on screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get link for yougov profile for each brand. This page has more robust sentiment data than the main list\n",
    "all_brands = driver.find_elements(By.XPATH, '//yg-rankings-entities-list//a')\n",
    "all_brands_links = [brand.get_attribute(\"href\") for brand in all_brands]\n",
    "above_95 = all_brands_links[0:343] # only include brands with fame >= 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\Documents\\learning\\Projects\\public_vs_twitter\\public_vs_twitter_env\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "temp_driver = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADIDAS', '100%', '67%', '6%', '26%']\n",
      "['ALDI', '100%', '72%', '9%', '19%']\n",
      "['CADBURY ROSES', '99%', '67%', '10%', '22%']\n",
      "['MALTESERS', '99%', '86%', '5%', '8%']\n",
      "['LINDT LINDOR MILK CHOCOLATE', '99%', '77%', '8%', '15%']\n",
      "['PRINGLES', '99%', '74%', '8%', '18%']\n",
      "['PIZZA HUT', '99%', '54%', '17%', '29%']\n",
      "['WEETABIX', '99%', '77%', '7%', '16%']\n",
      "['VOLVO', '99%', '55%', '7%', '37%']\n",
      "['KIT KAT FOUR FINGER', '99%', '80%', '7%', '13%']\n",
      "['TESCO', '99%', '76%', '6%', '18%']\n",
      "['DOVE', '99%', '76%', '2%', '21%']\n",
      "['GREGGS', '99%', '73%', '7%', '19%']\n",
      "['HEINZ TOMATO KETCHUP', '99%', '78%', '9%', '13%']\n",
      "['SKY', '99%', '49%', '23%', '27%']\n",
      "['TK MAXX', '99%', '55%', '12%', '32%']\n",
      "['FERRERO ROCHER', '99%', '65%', '16%', '18%']\n",
      "['SPORTS DIRECT', '99%', '45%', '23%', '32%']\n",
      "['SHELL', '99%', '31%', '31%', '38%']\n",
      "['PG TIPS', '99%', '70%', '8%', '21%']\n",
      "['VISA', '99%', '71%', '7%', '21%']\n",
      "['TWIX', '99%', '76%', '6%', '18%']\n",
      "['LEGO', '99%', '83%', '3%', '14%']\n",
      "['AUDI', '99%', '56%', '11%', '32%']\n",
      "['LLOYDS BANK', '99%', '43%', '14%', '42%']\n",
      "['CLARKS', '99%', '68%', '3%', '28%']\n",
      "['BBC', '99%', '61%', '19%', '19%']\n",
      "['YORKSHIRE TEA', '99%', '64%', '11%', '24%']\n",
      "['NATIONAL LOTTERY', '99%', '57%', '16%', '25%']\n",
      "['ARGOS', '99%', '75%', '6%', '18%']\n",
      "['SMARTIES', '99%', '78%', '4%', '17%']\n",
      "['CADBURY MILK CHOCOLATE MINI ROLLS', '99%', '70%', '10%', '18%']\n",
      "['7-UP', '99%', '67%', '11%', '22%']\n",
      "['CADBURY CARAMEL', '99%', '69%', '12%', '17%']\n",
      "[\"MCDONALD'S\", '99%', '51%', '23%', '25%']\n",
      "['WH SMITH', '99%', '58%', '7%', '33%']\n",
      "['PREMIER INN', '99%', '74%', '5%', '19%']\n",
      "['CADBURY CREME EGG', '99%', '62%', '15%', '22%']\n",
      "['DAIRY MILK', '99%', '86%', '5%', '8%']\n",
      "['APPLE', '99%', '60%', '17%', '22%']\n",
      "['CURRYS', '99%', '58%', '7%', '33%']\n",
      "['COCA-COLA', '99%', '61%', '20%', '18%']\n",
      "['DIET COKE', '99%', '55%', '21%', '23%']\n",
      "['LIDL', '99%', '72%', '9%', '18%']\n",
      "['SUBWAY', '99%', '59%', '15%', '24%']\n",
      "[\"MCVITIE'S MILK CHOCOLATE DIGESTIVES\", '99%', '82%', '4%', '12%']\n",
      "['PRIMARK', '99%', '58%', '13%', '28%']\n",
      "['NESCAFÉ', '99%', '65%', '13%', '21%']\n",
      "['MINI CHEDDARS', '99%', '72%', '10%', '17%']\n",
      "['RENAULT', '99%', '42%', '13%', '44%']\n",
      "['HARIBO', '99%', '62%', '15%', '21%']\n",
      "['EASYJET', '99%', '47%', '17%', '34%']\n",
      "['SCHWEPPES', '99%', '67%', '3%', '29%']\n",
      "['WALKERS SALT AND VINEGAR', '99%', '59%', '17%', '22%']\n",
      "['TWIX BISCUIT', '99%', '76%', '6%', '17%']\n",
      "['CHOCOLATE RAISINS', '98%', '66%', '17%', '15%']\n",
      "['EASTER EGGS', '98%', '70%', '7%', '21%']\n",
      "['NISSAN', '98%', '58%', '5%', '35%']\n",
      "['YORKIE MILK CHOCOLATE BAR', '98%', '69%', '8%', '22%']\n",
      "[\"UNCLE BEN'S\", '98%', '64%', '6%', '28%']\n",
      "['LONDON UNDERGROUND', '98%', '61%', '8%', '29%']\n",
      "['BURGER KING', '98%', '54%', '20%', '24%']\n",
      "['CADBURY', '98%', '82%', '5%', '11%']\n",
      "['KIT KAT CHUNKY', '98%', '72%', '9%', '17%']\n",
      "['QUALITY STREET', '98%', '68%', '10%', '20%']\n",
      "[\"ROWNTREE'S FRUIT GUMS\", '98%', '66%', '13%', '19%']\n",
      "['HONDA', '98%', '55%', '7%', '36%']\n",
      "['CADBURY DAIRY MILK FRUIT AND NUT', '98%', '60%', '26%', '12%']\n",
      "['BISTO', '98%', '80%', '4%', '14%']\n",
      "['COLGATE', '98%', '75%', '4%', '19%']\n",
      "['DETTOL', '98%', '74%', '4%', '20%']\n",
      "['COSTA COFFEE', '98%', '57%', '19%', '22%']\n",
      "['VASELINE', '98%', '72%', '1%', '24%']\n",
      "['NATIONWIDE BUILDING SOCIETY', '98%', '56%', '5%', '37%']\n",
      "['FANTA', '98%', '67%', '11%', '21%']\n",
      "['BOOTS', '98%', '76%', '5%', '17%']\n",
      "['IRN BRU', '98%', '51%', '22%', '26%']\n",
      "['AERO BUBBLES', '98%', '69%', '8%', '21%']\n",
      "[\"CADBURY'S MINI EGGS\", '98%', '75%', '8%', '15%']\n",
      "['WALKERS CHEESE AND ONION', '98%', '61%', '16%', '21%']\n",
      "['CADBURY ECLAIRS', '98%', '57%', '15%', '26%']\n",
      "['DORITOS', '98%', '63%', '11%', '25%']\n",
      "['LLOYDS PHARMACY', '98%', '64%', '6%', '28%']\n",
      "['WALKERS PRAWN COCKTAIL', '98%', '55%', '27%', '16%']\n",
      "['NIKE', '98%', '65%', '8%', '25%']\n",
      "['PEPSI', '98%', '60%', '13%', '25%']\n",
      "[\"M&M'S\", '98%', '67%', '8%', '23%']\n",
      "['VAUXHALL', '98%', '48%', '9%', '41%']\n",
      "[\"SAINSBURY'S\", '98%', '71%', '7%', '20%']\n",
      "['GALAXY', '98%', '75%', '5%', '17%']\n",
      "['NIVEA', '98%', '75%', '3%', '21%']\n",
      "['AA', '98%', '62%', '11%', '25%']\n",
      "['CHOCOLATE PEANUTS', '98%', '57%', '22%', '20%']\n",
      "['PAYPAL', '98%', '62%', '10%', '26%']\n",
      "['BMW', '98%', '55%', '11%', '33%']\n",
      "['TRAVELODGE', '98%', '56%', '10%', '32%']\n",
      "['DR PEPPER', '98%', '42%', '22%', '34%']\n",
      "['APPLE IPHONE', '98%', '54%', '24%', '20%']\n",
      "['7UP', '98%', '61%', '11%', '26%']\n",
      "['MERCEDES-BENZ', '98%', '61%', '9%', '28%']\n",
      "['PHILADELPHIA', '98%', '69%', '7%', '21%']\n",
      "['MARS', '98%', '74%', '8%', '16%']\n",
      "['NATIONAL EXPRESS', '98%', '53%', '8%', '37%']\n",
      "['NINTENDO', '98%', '67%', '4%', '27%']\n",
      "['WATERSTONES', '98%', '75%', '8%', '15%']\n",
      "['DIET PEPSI', '98%', '46%', '30%', '22%']\n",
      "['CADBURY BUTTONS', '98%', '79%', '5%', '14%']\n",
      "['HALIFAX', '98%', '53%', '9%', '35%']\n",
      "['SNICKERS', '98%', '62%', '16%', '20%']\n",
      "['PORSCHE', '98%', '61%', '6%', '31%']\n",
      "['TOBLERONE MILK CHOCOLATE', '98%', '72%', '11%', '14%']\n",
      "['KIT KAT', '98%', '81%', '5%', '11%']\n",
      "['LINDT CHOCOLATE GOLD RABBIT', '98%', '67%', '9%', '22%']\n",
      "['SANTANDER', '98%', '52%', '11%', '34%']\n",
      "[\"MCVITIE'S RICH TEA BISCUITS\", '98%', '53%', '15%', '30%']\n",
      "['AERO MILK CHOCOLATE', '98%', '76%', '6%', '16%']\n",
      "['HARRODS', '98%', '50%', '15%', '33%']\n",
      "['CADBURY FINGERS', '98%', '81%', '4%', '13%']\n",
      "['JAFFA CAKES', '98%', '76%', '9%', '12%']\n",
      "[\"JACOB'S CREAM CRACKER\", '98%', '75%', '5%', '18%']\n",
      "['ICELAND', '98%', '69%', '6%', '23%']\n",
      "['PIZZAEXPRESS', '98%', '56%', '13%', '29%']\n",
      "['WALKERS READY SALTED', '98%', '69%', '8%', '20%']\n",
      "['PLAYSTATION', '98%', '60%', '9%', '28%']\n",
      "['ANDREX', '98%', '75%', '2%', '21%']\n",
      "['SONY', '98%', '77%', '3%', '18%']\n",
      "['HOVIS', '98%', '72%', '3%', '22%']\n",
      "['TESCO EXPRESS', '98%', '70%', '6%', '22%']\n",
      "['HP SAUCE', '98%', '66%', '12%', '19%']\n",
      "['GALAXY MILK CHOCOLATE', '98%', '76%', '6%', '16%']\n",
      "['CADBURY DAIRY MILK WHOLE NUT', '98%', '64%', '16%', '17%']\n",
      "[\"NANDO'S\", '97%', '52%', '19%', '27%']\n",
      "[\"WALL'S\", '97%', '76%', '3%', '18%']\n",
      "['PRINGLES PRAWN COCKTAIL', '97%', '41%', '33%', '24%']\n",
      "['WALKERS', '97%', '83%', '4%', '11%']\n",
      "[\"MCVITIE'S\", '97%', '79%', '2%', '16%']\n",
      "['NUTELLA', '97%', '62%', '15%', '21%']\n",
      "['EUROMILLIONS', '97%', '48%', '16%', '33%']\n",
      "['POUNDLAND', '97%', '58%', '10%', '29%']\n",
      "['HOLIDAY INN', '97%', '51%', '5%', '41%']\n",
      "['POLO', '97%', '67%', '5%', '26%']\n",
      "['SUPERDRUG', '97%', '66%', '4%', '27%']\n",
      "['SPRITE', '97%', '61%', '7%', '29%']\n",
      "['BIRDS EYE', '97%', '72%', '7%', '19%']\n",
      "['20TH CENTURY FOX', '97%', '73%', '4%', '20%']\n",
      "['GALAXY MINSTRELS', '97%', '72%', '7%', '19%']\n",
      "['MAGNUM CLASSIC ICE CREAM', '97%', '82%', '4%', '11%']\n",
      "['VOLKSWAGEN', '97%', '60%', '8%', '30%']\n",
      "['CADBURY TWIRL', '97%', '79%', '3%', '14%']\n",
      "[\"MCVITIE'S ORIGINAL DIGESTIVE\", '97%', '69%', '6%', '23%']\n",
      "['CADBURY WISPA BAR', '97%', '77%', '7%', '14%']\n",
      "['SAMSUNG', '97%', '77%', '4%', '16%']\n",
      "[\"HELLMANN'S\", '97%', '68%', '10%', '20%']\n",
      "[\"TERRY'S CHOCOLATE ORANGE\", '97%', '74%', '13%', '11%']\n",
      "['NATIONAL RAIL', '97%', '43%', '17%', '38%']\n",
      "['SKITTLES', '97%', '61%', '16%', '21%']\n",
      "['HEINZ', '97%', '81%', '3%', '14%']\n",
      "['CADBURY GIANT BUTTONS', '97%', '75%', '6%', '16%']\n",
      "['KINDER', '97%', '58%', '15%', '24%']\n",
      "['RED BULL', '97%', '42%', '25%', '30%']\n",
      "[\"WERTHER'S ORIGINAL BUTTER CANDIES\", '97%', '58%', '13%', '26%']\n",
      "['AERO PEPPERMINT', '97%', '67%', '15%', '15%']\n",
      "['MÜLLER', '97%', '67%', '4%', '26%']\n",
      "['CADBURY MILK CHOCOLATE DIGESTIVES', '97%', '77%', '5%', '15%']\n",
      "['CELEBRATIONS', '97%', '72%', '6%', '19%']\n",
      "['DOUBLE DECKER', '97%', '63%', '11%', '23%']\n",
      "['MCCAIN', '97%', '71%', '4%', '22%']\n",
      "['GILLETTE', '97%', '64%', '4%', '29%']\n",
      "['HP', '97%', '62%', '2%', '34%']\n",
      "[\"JACOB'S\", '97%', '75%', '4%', '17%']\n",
      "['ASDA', '97%', '69%', '9%', '19%']\n",
      "['GUINNESS', '97%', '56%', '17%', '24%']\n",
      "['BRITISH GAS', '97%', '31%', '32%', '34%']\n",
      "['NESQUIK', '97%', '49%', '15%', '33%']\n",
      "['TIC TAC', '97%', '68%', '4%', '24%']\n",
      "[\"WERTHER'S ORIGINAL\", '97%', '69%', '7%', '20%']\n",
      "['KLEENEX', '97%', '74%', '1%', '22%']\n",
      "['M & S FOOD', '97%', '78%', '5%', '14%']\n",
      "['COCA-COLA ZERO', '97%', '46%', '21%', '29%']\n",
      "['LAND ROVER', '97%', '63%', '9%', '25%']\n",
      "['HÄAGEN-DAZS', '97%', '66%', '6%', '25%']\n",
      "['ROBINSONS', '97%', '76%', '3%', '18%']\n",
      "['DYSON', '97%', '60%', '12%', '24%']\n",
      "['DISNEY', '97%', '63%', '13%', '20%']\n",
      "['PEUGEOT', '97%', '45%', '12%', '40%']\n",
      "['TOSHIBA', '97%', '49%', '4%', '44%']\n",
      "['BARCLAYCARD', '97%', '45%', '14%', '38%']\n",
      "[\"MCVITIE'S DARK CHOCOLATE DIGESTIVES\", '97%', '64%', '17%', '15%']\n",
      "['CADBURY FUDGE', '97%', '59%', '13%', '24%']\n",
      "['FIZZY COLA BOTTLES', '97%', '49%', '26%', '22%']\n",
      "[\"M&M'S PEANUT\", '97%', '56%', '21%', '20%']\n",
      "[\"MCCOY'S\", '97%', '71%', '5%', '21%']\n",
      "['MAYNARDS WINE GUMS', '97%', '60%', '14%', '23%']\n",
      "['BRITISH AIRWAYS', '97%', '55%', '12%', '30%']\n",
      "['HEINZ BEANZ', '97%', '75%', '6%', '16%']\n",
      "['MILKY WAY', '97%', '59%', '10%', '28%']\n",
      "['KETTLE CHIPS SALT AND VINEGAR', '97%', '56%', '15%', '25%']\n",
      "['AQUAFRESH', '97%', '65%', '4%', '28%']\n",
      "['MILKYBAR', '97%', '65%', '13%', '19%']\n",
      "['PUMA', '97%', '53%', '9%', '35%']\n",
      "['CADBURY CARAMEL EGG', '97%', '56%', '20%', '20%']\n",
      "['JD SPORTS', '97%', '48%', '20%', '29%']\n",
      "['GUMMY BEARS', '97%', '54%', '15%', '28%']\n",
      "['MASTERCARD', '97%', '55%', '7%', '35%']\n",
      "['TWIX TWIN', '97%', '74%', '4%', '18%']\n",
      "['KFC', '97%', '55%', '22%', '20%']\n",
      "['NESTLÉ', '97%', '68%', '9%', '19%']\n",
      "['B&M', '97%', '63%', '8%', '26%']\n",
      "['HYUNDAI', '96%', '45%', '7%', '44%']\n",
      "['NESTLE MILKYBAR GIANT BUTTONS', '96%', '62%', '11%', '23%']\n",
      "['WEIGHT WATCHERS', '96%', '33%', '23%', '40%']\n",
      "['SPECSAVERS', '96%', '64%', '7%', '26%']\n",
      "['DOLMIO', '96%', '55%', '11%', '30%']\n",
      "['THE CO-OPERATIVE FOOD', '96%', '63%', '7%', '27%']\n",
      "['MR KIPLING', '96%', '71%', '5%', '21%']\n",
      "['CURLY WURLY', '96%', '64%', '14%', '19%']\n",
      "['LUCOZADE', '96%', '55%', '15%', '27%']\n",
      "['WARBURTONS', '96%', '78%', '4%', '15%']\n",
      "['TOYOTA', '96%', '56%', '6%', '34%']\n",
      "['HEINZ SALAD CREAM', '96%', '55%', '22%', '20%']\n",
      "['NUROFEN', '96%', '60%', '8%', '28%']\n",
      "['CALVIN KLEIN', '96%', '56%', '10%', '30%']\n",
      "['GALAXY CARAMEL', '96%', '68%', '10%', '18%']\n",
      "['CADBURY HEROES', '96%', '72%', '5%', '20%']\n",
      "['STELLA ARTOIS', '96%', '51%', '15%', '30%']\n",
      "['MILK TRAY', '96%', '70%', '8%', '18%']\n",
      "['ROLEX', '96%', '57%', '11%', '29%']\n",
      "['VIMTO', '96%', '53%', '15%', '28%']\n",
      "['AERO BUBBLES MILK CHOCOLATE', '96%', '69%', '8%', '19%']\n",
      "['THE CO-OPERATIVE GROUP', '96%', '64%', '6%', '27%']\n",
      "['HUGO BOSS', '96%', '49%', '12%', '34%']\n",
      "['HULA HOOPS', '96%', '72%', '9%', '15%']\n",
      "['CORNETTO CLASSIC ICE CREAM', '96%', '72%', '6%', '18%']\n",
      "['STARBURST', '96%', '59%', '11%', '26%']\n",
      "[\"JACOB'S BISCUITS FOR CHEESE\", '96%', '73%', '5%', '18%']\n",
      "[\"BEN & JERRY'S\", '96%', '59%', '12%', '25%']\n",
      "['FIAT', '96%', '42%', '15%', '39%']\n",
      "['FORD', '96%', '62%', '7%', '27%']\n",
      "['WAGON WHEELS', '96%', '60%', '11%', '24%']\n",
      "['HEAD & SHOULDERS', '96%', '54%', '9%', '32%']\n",
      "['JELLY BEANS', '96%', '52%', '18%', '26%']\n",
      "['POLO ORIGINAL MINTS', '96%', '58%', '10%', '28%']\n",
      "['THORNTONS', '96%', '68%', '5%', '23%']\n",
      "['FERRARI', '96%', '61%', '8%', '27%']\n",
      "['PC WORLD', '96%', '53%', '9%', '34%']\n",
      "['HEINEKEN', '96%', '43%', '17%', '36%']\n",
      "['AERO', '96%', '80%', '4%', '12%']\n",
      "['NATWEST', '96%', '40%', '14%', '42%']\n",
      "['AMERICAN EXPRESS', '96%', '35%', '18%', '43%']\n",
      "['HOOVER', '96%', '57%', '7%', '32%']\n",
      "['NEXT', '96%', '57%', '11%', '28%']\n",
      "[\"WALL'S ORIGINAL VIENNETTA ICE CREAM\", '96%', '64%', '9%', '23%']\n",
      "['TSB', '96%', '30%', '13%', '53%']\n",
      "['H&M', '96%', '51%', '9%', '36%']\n",
      "['BP', '96%', '31%', '29%', '36%']\n",
      "['CORNETTO', '96%', '69%', '5%', '22%']\n",
      "['ODEON', '96%', '58%', '6%', '32%']\n",
      "['AFTER EIGHT', '96%', '66%', '12%', '18%']\n",
      "[\"DOMINO'S PIZZA\", '96%', '56%', '14%', '26%']\n",
      "['HEINZ SERIOUSLY GOOD MAYONNAISE', '96%', '59%', '12%', '25%']\n",
      "['PANASONIC', '96%', '68%', '1%', '28%']\n",
      "['LONDON BUSES', '96%', '50%', '7%', '39%']\n",
      "['PHILIPS', '96%', '68%', '3%', '25%']\n",
      "['MAGNUM WHITE ICE CREAM', '96%', '62%', '14%', '20%']\n",
      "[\"LEVI'S\", '96%', '63%', '6%', '27%']\n",
      "['ESSO', '96%', '39%', '19%', '37%']\n",
      "[\"MCVITIE'S GINGER NUTS\", '96%', '59%', '13%', '23%']\n",
      "['XBOX', '96%', '42%', '17%', '37%']\n",
      "['STARBUCKS', '96%', '45%', '25%', '26%']\n",
      "['MARMITE', '96%', '39%', '33%', '23%']\n",
      "[\"MCVITIE'S HOBNOBS DARK CHOCOLATE\", '96%', '58%', '14%', '23%']\n",
      "['TREBOR EXTRA STRONG MINTS', '96%', '54%', '16%', '25%']\n",
      "['WALKERS QUAVERS CHEESE', '96%', '61%', '14%', '20%']\n",
      "['FLORA', '96%', '54%', '7%', '35%']\n",
      "['RYANAIR', '96%', '24%', '38%', '34%']\n",
      "['BASSETTS JELLY BABIES', '96%', '61%', '18%', '17%']\n",
      "['NESTLE ROLO', '96%', '69%', '6%', '21%']\n",
      "['APPLE MAC', '96%', '55%', '14%', '27%']\n",
      "['EMIRATES', '96%', '45%', '9%', '42%']\n",
      "['HOLLAND & BARRETT', '96%', '61%', '7%', '28%']\n",
      "['JAGUAR', '96%', '59%', '5%', '31%']\n",
      "['DURACELL', '96%', '75%', '1%', '19%']\n",
      "['SELFRIDGES', '96%', '45%', '9%', '41%']\n",
      "['MATALAN', '96%', '57%', '10%', '29%']\n",
      "['ORAL-B', '96%', '68%', '4%', '23%']\n",
      "['APPLE IPAD', '96%', '56%', '13%', '27%']\n",
      "['HSBC UK', '96%', '33%', '14%', '48%']\n",
      "['WILKINSON SWORD', '95%', '55%', '4%', '37%']\n",
      "['DISNEY STORE', '95%', '47%', '14%', '34%']\n",
      "['MALIBU', '95%', '53%', '16%', '26%']\n",
      "['DAIRYLEA', '95%', '63%', '9%', '23%']\n",
      "[\"KELLOGG'S\", '95%', '74%', '5%', '16%']\n",
      "['OREO', '95%', '54%', '17%', '24%']\n",
      "['BOOTS OPTICIANS', '95%', '53%', '6%', '37%']\n",
      "['BOURNVILLE DARK CHOCOLATE', '95%', '54%', '23%', '19%']\n",
      "['GUCCI', '95%', '40%', '22%', '33%']\n",
      "[\"JACK DANIEL'S\", '95%', '47%', '21%', '27%']\n",
      "['BACARDI', '95%', '51%', '12%', '32%']\n",
      "['WARNER BROS.', '95%', '74%', '3%', '18%']\n",
      "['RUSSELL HOBBS', '95%', '64%', '2%', '30%']\n",
      "['LINDT', '95%', '73%', '7%', '16%']\n",
      "['HMV', '95%', '56%', '6%', '33%']\n",
      "['WAITROSE', '95%', '51%', '11%', '33%']\n",
      "['HARIBO STARMIX', '95%', '61%', '15%', '19%']\n",
      "['BRANSTON', '95%', '61%', '8%', '26%']\n",
      "['FRUITTELLA', '95%', '63%', '11%', '22%']\n",
      "['MAGNUM', '95%', '81%', '3%', '11%']\n",
      "[\"MCVITIE'S HOBNOBS MILK CHOCOLATE\", '95%', '74%', '9%', '13%']\n",
      "['HALFORDS', '95%', '58%', '4%', '33%']\n",
      "['TOBLERONE', '95%', '75%', '6%', '15%']\n",
      "['RALPH LAUREN', '95%', '45%', '12%', '38%']\n",
      "[\"ROWNTREE'S FRUIT PASTILLES\", '95%', '72%', '6%', '17%']\n",
      "['RADOX', '95%', '65%', '3%', '26%']\n",
      "['MORRISONS', '95%', '67%', '6%', '22%']\n",
      "['BABYBEL', '95%', '54%', '14%', '27%']\n",
      "['WALT DISNEY PARKS AND RESORTS', '95%', '52%', '17%', '26%']\n",
      "['WOTSITS', '95%', '63%', '18%', '14%']\n",
      "['COLA BOTTLES', '95%', '61%', '16%', '18%']\n",
      "['HÄAGEN-DAZS VANILLA ICE CREAM', '95%', '63%', '7%', '25%']\n",
      "['VIRGIN HOLIDAYS', '95%', '43%', '12%', '40%']\n",
      "['PLAYBOY', '95%', '26%', '34%', '35%']\n",
      "['CADBURY CHOCOLATE MINI FINGERS', '95%', '74%', '6%', '14%']\n",
      "['PIZZA HUT DELIVERY', '95%', '44%', '16%', '35%']\n",
      "['ROLO', '95%', '75%', '4%', '16%']\n",
      "['TOFFEE CRISP', '95%', '69%', '10%', '16%']\n",
      "['THOMAS COOK', '95%', '35%', '15%', '45%']\n",
      "[\"MCVITIE'S CHEDDARS CHEESE BISCUITS\", '95%', '62%', '10%', '23%']\n",
      "['CENTER PARCS', '95%', '44%', '14%', '37%']\n",
      "['SHERBERT FOUNTAIN', '95%', '47%', '20%', '27%']\n",
      "['GALAXY RIPPLE', '95%', '71%', '6%', '17%']\n",
      "['KINDER BUENO MILK AND HAZELNUT CHOCOLATE', '95%', '54%', '18%', '23%']\n",
      "['CASIO', '95%', '54%', '7%', '33%']\n",
      "['PRINGLES SALT AND VINEGAR', '95%', '56%', '19%', '20%']\n",
      "['THE BODY SHOP', '95%', '56%', '8%', '31%']\n",
      "['WILLIAM HILL', '95%', '24%', '38%', '32%']\n",
      "['REEBOK', '95%', '51%', '10%', '34%']\n",
      "['BENTLEY', '95%', '61%', '6%', '28%']\n",
      "['EVIAN', '95%', '55%', '8%', '31%']\n",
      "['BOUNTY', '95%', '52%', '20%', '22%']\n",
      "['HÄAGEN-DAZS COOKIES AND CREAM ICE CREAM', '95%', '50%', '16%', '29%']\n",
      "['LINDT CHOCOLATE GOLD TEDDY BEAR', '95%', '66%', '8%', '21%']\n",
      "['WALKERS BAKED READY SALTED', '95%', '60%', '9%', '26%']\n",
      "['LG', '95%', '60%', '4%', '31%']\n"
     ]
    }
   ],
   "source": [
    "# loop through each url and scrape sentiment data\n",
    "brands_sentiment = []\n",
    "for link in above_95:\n",
    "    temp_driver.get(link)\n",
    "    time.sleep(10) # Selenium risks recording incomplete data if there is not sufficient time for the page to load\n",
    "    just_sentiment = [item.text for item in temp_driver.find_elements(By.CLASS_NAME, \"value\")]\n",
    "    just_name = temp_driver.find_element(By.CLASS_NAME, \"entity-name\").text\n",
    "    brand_sentiment_and_name = [just_name, just_sentiment[0], just_sentiment[1], just_sentiment[2], just_sentiment[3]]\n",
    "    brands_sentiment.append(brand_sentiment_and_name)\n",
    "    print(brand_sentiment_and_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing percent sign and casting as float\n",
    "for brand in brands_sentiment:\n",
    "    brand[1] = float( str.replace(brand[1], '%', \"\") )\n",
    "    brand[2] = float( str.replace(brand[2], '%', \"\") )\n",
    "    brand[3] = float( str.replace(brand[3], '%', \"\") )\n",
    "    brand[4] = float( str.replace(brand[4], '%', \"\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving survey data to csv\n",
    "df_public = pd.DataFrame(brands_sentiment)\n",
    "df_public.columns = ['name', 'fame', 'positive', 'negative', 'neutral']\n",
    "df_public.to_csv('public_data.csv')\n",
    "driver.close()\n",
    "temp_driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Data\n",
    "\n",
    "I hand collected the twitter handles for companies with fame greater than 95%. This data is stored on the famous_brands.csv file.\n",
    "\n",
    "The inclusion criteria is as follows:\n",
    "- The brand's Twitter handle must contain the prefix or suffix 'UK'.\n",
    "- A subsidiary brand like Peanut M&M will have its larger brand included (M&M), unless there exists an account for the subsidiary brand.\n",
    "- If multiple accounts claiming to be the twitter account of a brand are shown in the search bar, and none of them are verified, no account will be added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "famous_brands_with_twitter_df = pd.read_csv('famous_brands_with_twitter.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying that each brand in famous_brands_with_twitter_df is in public_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "brand_names_with_twitter = list(famous_brands_with_twitter_df['name'])\n",
    "public_brand_names = list(df_public['name'])\n",
    "\n",
    "not_included = []\n",
    "for brand in brand_names_with_twitter:\n",
    "    if brand.upper() not in public_brand_names:\n",
    "        not_included.append(brand)\n",
    "\n",
    "print(not_included)\n",
    "print(len(brand_names_with_twitter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Twitter handle variable to survey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>fame</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>twitter_uk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADIDAS</td>\n",
       "      <td>100.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>adidasUK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALDI</td>\n",
       "      <td>100.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>AldiUK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MALTESERS</td>\n",
       "      <td>99.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>MaltesersUK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PRINGLES</td>\n",
       "      <td>99.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Pringles_UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PIZZA HUT</td>\n",
       "      <td>99.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>pizzahutuk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>CASIO</td>\n",
       "      <td>95.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>CasioMusicUK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>THE BODY SHOP</td>\n",
       "      <td>95.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>TheBodyShopUK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>BOUNTY</td>\n",
       "      <td>95.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>BountyUK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>HÄAGEN-DAZS COOKIES AND CREAM ICE CREAM</td>\n",
       "      <td>95.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>haagendazsuk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>LG</td>\n",
       "      <td>95.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>LGUK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        name   fame  positive  negative  \\\n",
       "0                                     ADIDAS  100.0      67.0       6.0   \n",
       "1                                       ALDI  100.0      72.0       9.0   \n",
       "2                                  MALTESERS   99.0      86.0       5.0   \n",
       "3                                   PRINGLES   99.0      74.0       8.0   \n",
       "4                                  PIZZA HUT   99.0      54.0      17.0   \n",
       "..                                       ...    ...       ...       ...   \n",
       "118                                    CASIO   95.0      54.0       7.0   \n",
       "119                            THE BODY SHOP   95.0      56.0       8.0   \n",
       "120                                   BOUNTY   95.0      52.0      20.0   \n",
       "121  HÄAGEN-DAZS COOKIES AND CREAM ICE CREAM   95.0      50.0      16.0   \n",
       "122                                       LG   95.0      60.0       4.0   \n",
       "\n",
       "     neutral     twitter_uk  \n",
       "0       26.0       adidasUK  \n",
       "1       19.0         AldiUK  \n",
       "2        8.0    MaltesersUK  \n",
       "3       18.0    Pringles_UK  \n",
       "4       29.0     pizzahutuk  \n",
       "..       ...            ...  \n",
       "118     33.0   CasioMusicUK  \n",
       "119     31.0  TheBodyShopUK  \n",
       "120     22.0       BountyUK  \n",
       "121     29.0   haagendazsuk  \n",
       "122     31.0           LGUK  \n",
       "\n",
       "[123 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Joining twitter handle to dataframe with survey sentiment data\n",
    "famous_brands_with_twitter_df['name'] = famous_brands_with_twitter_df['name'].apply(lambda x: str.upper(x))\n",
    "public_with_twitter_df = pd.merge(df_public, famous_brands_with_twitter_df, on='name')\n",
    "public_with_twitter_df.drop(columns='Unnamed: 0', inplace=True)\n",
    "public_with_twitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_with_twitter_df.to_csv('merged_corp_public.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\Documents\\learning\\Projects\\public_vs_twitter\\public_vs_twitter_env\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "advanced_search_url = 'https://twitter.com/search-advanced?lang=en'\n",
    "\n",
    "def q2_search_url(handle):\n",
    "    return f'https://twitter.com/search?lang=en&q=(to%3A{handle})%20until%3A2022-06-30%20since%3A2022-04-01&src=typed_query'\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code does three things for each brand\n",
    "1. Search for tweets sent in reply to the brand during Q2 period\n",
    "2. Scrape all these tweets\n",
    "3. Create a dictionary with handle as key, with a list of all tweets to that brand as the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_handles = pd.read_csv('merged_corp_public.csv')\n",
    "twitter_handles = list(twitter_handles['twitter_uk'])\n",
    "\n",
    "brand_replies = {}\n",
    "for handle in twitter_handles:\n",
    "    driver.get(q2_search_url(handle))\n",
    "    time.sleep(10) # allow sufficient time for page to fully load\n",
    "    # After scrolling some distance down, earlier tweets will not be accessable from the HTML file.\n",
    "    # To account for this, all available tweets will be scraped after every page down, and duplicate tweets will be removed later\n",
    "    tweets_with_duplicates = []\n",
    "    for pg_down in range(50):\n",
    "        #scrape text from tweets\n",
    "        tweet_objects = driver.find_elements(By.XPATH, '//div[@data-testid=\"tweetText\"]')    \n",
    "        for tweet in tweet_objects:\n",
    "            tweets_with_duplicates.append(tweet.text)\n",
    "        repeat_page_down(1, driver)\n",
    "    # including only unique tweets\n",
    "    tweets = []\n",
    "    for tweet in tweets_with_duplicates:\n",
    "        if tweet not in tweets:\n",
    "            tweets.append(tweet)\n",
    "    brand_replies.update({handle: tweets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"brand_replies.json\", \"w\") as file:\n",
    "    json.dump(brand_replies, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning <a name=\"DataCleaning\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying every handle in the twitter replies corresponds with a handle in the merged_corp_public dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "bool_test = []\n",
    "for brand in brand_replies.keys():\n",
    "    bool_test.append(brand in list(twitter_handles))\n",
    "\n",
    "print(False in bool_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying correct data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name           object\n",
       "fame          float64\n",
       "positive      float64\n",
       "negative      float64\n",
       "neutral       float64\n",
       "twitter_uk     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "public_with_twitter_df.dtypes #correct data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing Text to Remove Symbols and Non-English Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('brand_replies.json') as file:\n",
    "    brand_replies = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = []\n",
    "number_of_characters = 0\n",
    "for brand in brand_replies:\n",
    "    for reply in brand_replies[brand]:\n",
    "        for char in reply:\n",
    "            symbols.append(char)\n",
    "            number_of_characters += 1\n",
    "symbols = set(symbols)\n",
    "\n",
    "english_letters_and_space = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', ' ']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_letters_and_space = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', ' ']\n",
    "\n",
    "# replace newline with space\n",
    "for brand in brand_replies:\n",
    "    replies_no_symbols = []\n",
    "    for reply in brand_replies[brand]:\n",
    "        reply = reply.replace('\\n', \"\")\n",
    "        replies_no_symbols.append(reply)\n",
    "    brand_replies.update({brand : replies_no_symbols})  \n",
    "\n",
    "# removing all non-english symbols from tweets, allowing for greater text processing\n",
    "for brand in brand_replies:\n",
    "    replies_no_symbols = []\n",
    "    for reply in brand_replies[brand]:\n",
    "        english_reply = reply # initializes vairable that will hold a reply containing only english letters\n",
    "        for symbol in symbols:\n",
    "            if symbol not in english_letters_and_space:\n",
    "                english_reply = english_reply.replace(symbol, \"\")\n",
    "        replies_no_symbols.append(english_reply)\n",
    "    brand_replies.update({brand : replies_no_symbols})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"brand_replies.json\", \"w\") as file:\n",
    "    json.dump(brand_replies, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Survey data has values within valid range of 0 and 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "num_of_fame_violations = len(public_with_twitter_df[ (public_with_twitter_df[\"fame\"] > 100) | (public_with_twitter_df[\"fame\"] < 0) ])\n",
    "num_of_positive_violations = len(public_with_twitter_df[ (public_with_twitter_df[\"positive\"] > 100) | (public_with_twitter_df[\"positive\"] < 0) ])\n",
    "num_of_negative_violations = len(public_with_twitter_df[ (public_with_twitter_df[\"negative\"] > 100) | (public_with_twitter_df[\"negative\"] < 0) ])\n",
    "num_of_neutral_violations = len(public_with_twitter_df[ (public_with_twitter_df[\"neutral\"] > 100) | (public_with_twitter_df[\"neutral\"] < 0) ])\n",
    "num_of_total_violations = num_of_fame_violations + num_of_positive_violations +num_of_negative_violations + num_of_neutral_violations\n",
    "print(num_of_total_violations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, Fame = Positive + Negative + Neutral. However as shown below, rounding/truncating causes the formula to occasionally be slightly off. Beyond setting the inclusion criteria, the fame variable plays no role in the remainder of the analysis. As such the violation poses no threat to analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "1.0\n",
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "print(len(public_with_twitter_df[ public_with_twitter_df['fame'] != (public_with_twitter_df[\"positive\"] + public_with_twitter_df[\"negative\"] + public_with_twitter_df[\"neutral\"]) ])) # many rows violate the fame = positive + negative + neutral\n",
    "# testing if discrepency is due to rounding\n",
    "rounding_test = pd.Series(public_with_twitter_df['fame'] - (public_with_twitter_df[\"positive\"] + public_with_twitter_df[\"negative\"] + public_with_twitter_df[\"neutral\"]))\n",
    "print(max(rounding_test))\n",
    "print(min(rounding_test))\n",
    "# since the difference between public_with_twitter_df['fame'] and (public_with_twitter_df[\"positive\"] + public_with_twitter_df[\"negative\"] + public_with_twitter_df[\"neutral\"]) is at most 1 point away, and the original data did not have\n",
    "# fractions, the difference is most likely due to rounding/truncating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "There were no missing values in the survey data. There were multiple brands that did not get responses to their tweets in Q2 2022. Since no sentiment analysis could be performed on these brands, they were dropped from analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name          0\n",
       "fame          0\n",
       "positive      0\n",
       "negative      0\n",
       "neutral       0\n",
       "twitter_uk    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for survey data\n",
    "public_with_twitter_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tweets\n",
    "num_of_brands_no_replies = 0\n",
    "brands_no_replies = []\n",
    "for brand in brand_replies:\n",
    "    if brand_replies[brand] == []:\n",
    "        num_of_brands_no_replies += 1\n",
    "        brands_no_replies.append(brand)\n",
    "\n",
    "# As no twitter data exists to estimate sentiment distribution, these brands will be dropped\n",
    "for brand in brands_no_replies:\n",
    "    del brand_replies[brand]\n",
    "\n",
    "# brands_no_replies in public_with_twitter_df[\"twitter_uk\"]\n",
    "just_handles = public_with_twitter_df[\"twitter_uk\"]\n",
    "public_with_twitter_df = public_with_twitter_df[ public_with_twitter_df['twitter_uk'].apply(lambda handle: False if (handle in brands_no_replies) else True) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out Brands with less than 10 Replies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding Brands with less than 10 replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('brand_replies.json') as file:\n",
    "    brand_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_handles , brand_replies = list(brand_dict.keys()), list(brand_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_counter_per_brand = []\n",
    "for brand in brand_replies:\n",
    "    counter = 0\n",
    "    for tweet in brand:\n",
    "        counter += 1\n",
    "    reply_counter_per_brand.append(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining twitter reply count to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter_vs_public.csv')\n",
    "df_to_merge = pd.DataFrame(list(zip(brand_handles,reply_counter_per_brand)), columns=['twitter_uk', 'reply_counter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_to_merge, on='twitter_uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "102\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "print(len(df[ df['reply_counter'] >= 10 ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[ df['reply_counter'] >= 10 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('twitter_vs_public.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis <a name=\"Analysis\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flair is a pretrained neural network used for sentiment classification. As demonstrated below, Flair has a high accuracy for predicting the sentiment of amazon reviews. One star reviews were used for negative sentiment, five star reviews were used for positive sentiment. 10000 reviews from each category were randomly chosen and passed through the model for classification. The model demonstrated an accuracy and precision greater than 90%.\n",
    "\n",
    "Flair classifies data as either 'Positive' or 'Negative'. However, some text may be neutral. This is examined in greater detail in the appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon and Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment:\n",
    "    negative = 'NEGATIVE'\n",
    "    positive = 'POSITIVE'\n",
    "    neutral = 'NEUTRAL'\n",
    "\n",
    "class Review:\n",
    "    def __init__(self, text, rating):\n",
    "        self.text = text\n",
    "        self.rating = int(rating)\n",
    "        self.sentiment = self.get_sentiment()\n",
    "    \n",
    "    def get_sentiment(self):\n",
    "        if self.rating == 5:\n",
    "            return Sentiment.positive\n",
    "        elif self.rating == 1:\n",
    "            return Sentiment.negative\n",
    "        elif self.rating == 3:\n",
    "            return Sentiment.neutral\n",
    "        else: return \"DROP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data\n",
    "amazon_data = []\n",
    "with open('kcore_5.json') as file:\n",
    "    num = 0\n",
    "    for line in file:\n",
    "        review = json.loads(line)\n",
    "        amazon_data.append(Review(review[\"reviewText\"], review[\"overall\"]))\n",
    "        num += 1\n",
    "        if num >= 250000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of 1 star reviews:  12706\n",
      "number of 5 star reviews:  142122\n"
     ]
    }
   ],
   "source": [
    "one_star_reviews = []\n",
    "five_star_reviews = []\n",
    "for review in amazon_data:\n",
    "    if review.text == \"\":\n",
    "        amazon_data.remove(review)\n",
    "        continue\n",
    "    if review.rating == 1:\n",
    "        one_star_reviews.append(review)\n",
    "    elif review.rating == 5:\n",
    "        five_star_reviews.append(review)\n",
    "\n",
    "print(\"number of 1 star reviews: \",len(one_star_reviews))\n",
    "print(\"number of 5 star reviews: \",len(five_star_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of observations:  20000\n"
     ]
    }
   ],
   "source": [
    "#selecting a subset of data for testing flair model\n",
    "random.seed(0)\n",
    "ten_k_one_star = random.sample(one_star_reviews,10000)\n",
    "ten_k_five_star = random.sample(five_star_reviews,10000)\n",
    "aggregated_samples = []\n",
    "for sample in [ten_k_one_star, ten_k_five_star]:\n",
    "    for draw in sample:\n",
    "        aggregated_samples.append(draw)\n",
    "\n",
    "print(\"number of observations: \",len(aggregated_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_reviews = []\n",
    "for review in aggregated_samples:\n",
    "    text_reviews.append(review.text)\n",
    "\n",
    "numerical_reviews = []\n",
    "for review in aggregated_samples:\n",
    "    numerical_reviews.append(review.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-29 09:51:49,737 loading file C:\\Users\\franc\\.flair\\models\\sentiment-en-mix-distillbert_4.pt\n"
     ]
    }
   ],
   "source": [
    "classifier = TextClassifier.load('en-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sentiment = []\n",
    "\n",
    "for review in text_reviews:\n",
    "    sentence = Sentence(review)\n",
    "    classifier.predict(sentence)\n",
    "\n",
    "    # taking prediction from object form into string\n",
    "    temp_string = str.split(sentence.labels[0].__str__(), '→')[1]\n",
    "    temp_string = str.split(temp_string,'(')[0].strip()\n",
    "\n",
    "\n",
    "    predicted_sentiment.append(temp_string) #POSITIVE and NEGATIVE options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_sentiment_data = pd.DataFrame(list(zip(text_reviews, numerical_reviews, predicted_sentiment)), columns=[\"text_reviews\", \"true_stars_reviews\", \"predicted_sentiment\"])\n",
    "    \n",
    "\n",
    "def stars_to_words(series):\n",
    "    if series == 1:\n",
    "        return 'NEGATIVE'\n",
    "    if series == 3:\n",
    "        return 'NEUTRAL'\n",
    "    if series == 5:\n",
    "        return 'POSITIVE'\n",
    "\n",
    "\n",
    "amazon_sentiment_data['true_sentiment_reviews'] = amazon_sentiment_data['true_stars_reviews'].apply(lambda x: stars_to_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score:  0.9477\n",
      "Precision Score:  0.9690905280804694\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Accuracy Score: \",accuracy_score(amazon_sentiment_data[\"true_sentiment_reviews\"], amazon_sentiment_data[\"predicted_sentiment\"]))\n",
    "print(\"Precision Score: \",precision_score(amazon_sentiment_data[\"true_sentiment_reviews\"], amazon_sentiment_data[\"predicted_sentiment\"], pos_label=Sentiment.positive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Twitter Reply Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code transforms the twitter data to have one response on each row, rather than one row containing every reply to a brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('brand_replies.json') as file:\n",
    "    brand_replies = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8115\n",
      "8104\n"
     ]
    }
   ],
   "source": [
    "brand_list = []\n",
    "reply_list = []\n",
    "sentiment_list = []\n",
    "\n",
    "for brand in brand_replies:\n",
    "    for reply in brand_replies[brand]:\n",
    "        brand_list.append(brand)\n",
    "        reply_list.append(reply)\n",
    "        \n",
    "\n",
    "flat_reply = pd.DataFrame(list(zip(brand_list, reply_list)), columns=['brand', 'reply'])\n",
    "print(len(flat_reply))\n",
    "flat_reply = flat_reply[ flat_reply['reply'] != \"\"]\n",
    "flat_reply = flat_reply[ flat_reply['reply'] != \" \"]\n",
    "print(len(flat_reply))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis, Twitter\n",
    "\n",
    "applying the Flair model to scraped tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sentiment = []\n",
    "\n",
    "for reply in flat_reply['reply']:\n",
    "    sentence = Sentence(reply)\n",
    "    classifier.predict(sentence)\n",
    "    # taking prediction from object form into string\n",
    "    temp_string = str.split(sentence.labels[0].__str__(), '→')[1]\n",
    "    temp_string = str.split(temp_string,'(')[0].strip()\n",
    "\n",
    "    predicted_sentiment.append(temp_string) #POSITIVE and NEGATIVE options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sentiment_dummy = []\n",
    "positive_prediction = []\n",
    "negative_prediction = []\n",
    "\n",
    "for prediction in predicted_sentiment:\n",
    "    if prediction == Sentiment.positive:\n",
    "        predicted_sentiment_dummy.append(1)\n",
    "        positive_prediction.append(1)\n",
    "        negative_prediction.append(0)\n",
    "    if prediction == Sentiment.negative:\n",
    "        predicted_sentiment_dummy.append(0)\n",
    "        negative_prediction.append(1)\n",
    "        positive_prediction.append(0)\n",
    "\n",
    "#flat_reply[\"classification\"] = predicted_sentiment_dummy\n",
    "flat_reply[\"positive_prediction\"] = positive_prediction\n",
    "flat_reply[\"negative_prediction\"] = negative_prediction\n",
    "flat_reply[\"total_prediction\"] = flat_reply['positive_prediction'] + flat_reply['negative_prediction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating predicted sentiment data to merge with survey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_twitter = flat_reply.groupby('brand').mean(['positive_prediction', 'negative_prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_twitter.to_csv('brand_classification.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging survey data and classification summary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_twitter = pd.read_csv('brand_classification.csv')\n",
    "merged_corp_public_df = pd.read_csv('merged_corp_public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n",
      "123\n",
      "115\n"
     ]
    }
   ],
   "source": [
    "#creating shared column name\n",
    "\n",
    "grouped_twitter.rename(columns={'brand': 'twitter_uk'}, inplace=True)\n",
    "\n",
    "print(len(grouped_twitter))\n",
    "print(len(merged_corp_public_df))\n",
    "df = pd.merge(merged_corp_public_df, grouped_twitter, on='twitter_uk', how='inner')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error in data collection, Tescos and Tesco Express were both added when they shared the same twitter handle. Tesco Express is dropped below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[ df['twitter_uk'] == 'TescosUK' ]\n",
    "df = df[ df['name'] != 'TESCO EXPRESS' ] # 1 obs was removed, error in data collection, they share the same twitter handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['fame', 'neutral', 'Unnamed: 0'], inplace=True) #not necessary for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'positive': 'positive_survey', 'negative': 'negative_survey'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('twitter_vs_public.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating additional Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 additional variables were created as defined below:\n",
    "1. like_50_survey = 1 if at least 50% of respondents had a positive impression of a brand, 0 otherwise\n",
    "2. like_50_prediction = 1 if replies to a brand was at least 50% positive, 0 otherwise\n",
    "3. twitter_more_popular = 1 if a brand had a greater percent of positive responses than positive impression from survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter_vs_public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['like_50_survey'] = df['positive_survey'].apply(lambda x: 1 if (x >= 50) else 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['like_50_prediction'] = df['positive_prediction'].apply(lambda x: 1 if (x >= .50) else 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['twitter_more_positive'] = (df['positive_prediction']*100 > df['positive_survey']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('twitter_vs_public.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results <a name=\"Results\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%html\n",
    "<div class='tableauPlaceholder' id='viz1661448710888' style='position: relative'><noscript><a href='#'><img alt='Dashboard 1 ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Tw&#47;TwittervsPublic&#47;Dashboard1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='TwittervsPublic&#47;Dashboard1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Tw&#47;TwittervsPublic&#47;Dashboard1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='language' value='en-US' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1661448710888');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='1526px';vizElement.style.height='878px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix <a name=\"Appendix\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentiment classification model only classifies a string as positive or negative. As such tweets that were neutral in sentiment were classified as positive or negative. This section aims to estimates the classification distribution of neutral text via amazon review data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data\n",
    "amazon_data = []\n",
    "with open('kcore_5.json') as file:\n",
    "    num = 0\n",
    "    for line in file:\n",
    "        review = json.loads(line)\n",
    "        amazon_data.append(Review(review[\"reviewText\"], review[\"overall\"]))\n",
    "        num += 1\n",
    "        if num >= 250000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of 3 star reviews:  26019\n"
     ]
    }
   ],
   "source": [
    "three_star_reviews = []\n",
    "for review in amazon_data:\n",
    "    if review.text == \"\":\n",
    "        amazon_data.remove(review)\n",
    "        continue\n",
    "    if review.rating == 3:\n",
    "        three_star_reviews.append(review)\n",
    "\n",
    "\n",
    "print(\"number of 3 star reviews: \",len(three_star_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#selecting a subset of data for testing flair model\n",
    "random.seed(0)\n",
    "ten_k_three_star = random.sample(three_star_reviews,10000)\n",
    "aggregated_samples = []\n",
    "for sample in [ten_k_three_star]:\n",
    "    for draw in sample:\n",
    "        aggregated_samples.append(draw)\n",
    "\n",
    "len(aggregated_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_reviews = []\n",
    "for review in aggregated_samples:\n",
    "    text_reviews.append(review.text)\n",
    "\n",
    "numerical_reviews = []\n",
    "for review in aggregated_samples:\n",
    "    numerical_reviews.append(review.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-29 10:41:57,073 loading file C:\\Users\\franc\\.flair\\models\\sentiment-en-mix-distillbert_4.pt\n"
     ]
    }
   ],
   "source": [
    "classifier = TextClassifier.load('en-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " 'POSITIVE',\n",
       " 'NEGATIVE',\n",
       " 'POSITIVE',\n",
       " ...]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_sentiment = []\n",
    "\n",
    "for review in text_reviews:\n",
    "    sentence = Sentence(review)\n",
    "    classifier.predict(sentence)\n",
    "\n",
    "    # taking prediction from object form into string\n",
    "    temp_string = str.split(sentence.labels[0].__str__(), '→')[1]\n",
    "    temp_string = str.split(temp_string,'(')[0].strip()\n",
    "\n",
    "\n",
    "    predicted_sentiment.append(temp_string) #POSITIVE and NEGATIVE options\n",
    "\n",
    "predicted_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_reviews</th>\n",
       "      <th>true_stars_reviews</th>\n",
       "      <th>predicted_sentiment</th>\n",
       "      <th>true_sentiment_reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I read a review of this in the Marine Corp Gaz...</td>\n",
       "      <td>3</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I love me some Kingsolver, but this book was a...</td>\n",
       "      <td>3</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I have discovered Elizabeth George years ago w...</td>\n",
       "      <td>3</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In EXECUTIVE ORDERS, Tom Clancy continues the ...</td>\n",
       "      <td>3</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Interesting, but the author really needs a goo...</td>\n",
       "      <td>3</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>I found this hard to write because I am such a...</td>\n",
       "      <td>3</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Kaye Gibbons has a very breezy, readable style...</td>\n",
       "      <td>3</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>This was a true story and what a sad story it ...</td>\n",
       "      <td>3</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Let me start by saying that I like photos in m...</td>\n",
       "      <td>3</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>Jodi Picoult's &amp;quot;The Pact&amp;quot; is reminis...</td>\n",
       "      <td>3</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>NEUTRAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           text_reviews  true_stars_reviews  \\\n",
       "0     I read a review of this in the Marine Corp Gaz...                   3   \n",
       "1     I love me some Kingsolver, but this book was a...                   3   \n",
       "2     I have discovered Elizabeth George years ago w...                   3   \n",
       "3     In EXECUTIVE ORDERS, Tom Clancy continues the ...                   3   \n",
       "4     Interesting, but the author really needs a goo...                   3   \n",
       "...                                                 ...                 ...   \n",
       "9995  I found this hard to write because I am such a...                   3   \n",
       "9996  Kaye Gibbons has a very breezy, readable style...                   3   \n",
       "9997  This was a true story and what a sad story it ...                   3   \n",
       "9998  Let me start by saying that I like photos in m...                   3   \n",
       "9999  Jodi Picoult's &quot;The Pact&quot; is reminis...                   3   \n",
       "\n",
       "     predicted_sentiment true_sentiment_reviews  \n",
       "0               NEGATIVE                NEUTRAL  \n",
       "1               NEGATIVE                NEUTRAL  \n",
       "2               POSITIVE                NEUTRAL  \n",
       "3               NEGATIVE                NEUTRAL  \n",
       "4               NEGATIVE                NEUTRAL  \n",
       "...                  ...                    ...  \n",
       "9995            NEGATIVE                NEUTRAL  \n",
       "9996            POSITIVE                NEUTRAL  \n",
       "9997            NEGATIVE                NEUTRAL  \n",
       "9998            NEGATIVE                NEUTRAL  \n",
       "9999            NEGATIVE                NEUTRAL  \n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_sentiment_data = pd.DataFrame(list(zip(text_reviews, numerical_reviews, predicted_sentiment)), columns=[\"text_reviews\", \"true_stars_reviews\", \"predicted_sentiment\"])\n",
    "    \n",
    "\n",
    "def stars_to_words(series):\n",
    "    if series == 1:\n",
    "        return 'NEGATIVE'\n",
    "    if series == 3:\n",
    "        return 'NEUTRAL'\n",
    "    if series == 5:\n",
    "        return 'POSITIVE'\n",
    "\n",
    "\n",
    "amazon_sentiment_data['true_sentiment_reviews'] = amazon_sentiment_data['true_stars_reviews'].apply(lambda x: stars_to_words(x))\n",
    "amazon_sentiment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the 3 star reviews, 74% of them were assigned as negative and 26% were assigned positive. From personal experience and opinion, 3 star amazon reviews tend to skew negative, which may partially account for the difference, but is nevertheless important to keep in mind when considering this analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of 3 star reviews predicted positive are:  0.2578\n",
      "the number of 3 star reviews predicted negative are:  0.7422\n"
     ]
    }
   ],
   "source": [
    "only_threes = amazon_sentiment_data[ amazon_sentiment_data['true_stars_reviews'] == 3 ]\n",
    "print('the number of 3 star reviews predicted positive are: ' ,len( only_threes[ only_threes['predicted_sentiment'] == Sentiment.positive ])/len(only_threes))\n",
    "print('the number of 3 star reviews predicted negative are: ' ,len( only_threes[ only_threes['predicted_sentiment'] == Sentiment.negative ])/len(only_threes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 ('public_vs_twitter_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "203103cac922ad4d21d6ac9a664b6fce2c05f2559d9918e7761fb6ac03e36d01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
