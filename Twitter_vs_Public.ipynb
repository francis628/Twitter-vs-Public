{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter vs. Public"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of Contents\n",
    "- [Introduction](#Introduction)\n",
    "- [Data Collection](#DataCollection)\n",
    "- [Data Cleaning](#DataCleaning)\n",
    "- [Analysis](#Analysis)\n",
    "- [Results](#Results)\n",
    "- [Appendix](#Appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"Introduction\"></a>\n",
    "\n",
    "This work seeks to provide evidence on the differences in sentiment (Positive/Negative) between perceptions shared on twitter versus perceptions held by the public at large. Specifically, this analysis will be limited to sentiment of famous brands between the two groups. The public's sentiment will be approximated by brand specific survey data collected by YouGov. Whereas for Twitter data, tweets responding to these brands will be scraped and classified as either positive or negative. \n",
    "\n",
    "It is worth noting that the publicly available YouGov brand data is country specific. To ensure the validity of the data, only UK data will be considered. The UK was chosen for three reasons. First, in most countries, large brands have region specific twitter accounts (i.e. Pizza Hut has the @pizzahutuk twitter account). However, often times for the United States, brands opt not to have a US specific account (i.e. the American Pizza Hut account is simply @pizzahut). It is fair to assume that someone responding to a brand's regional account has a high likelyhood of being from that region. However, it is not fair to assume that tweets directed at a brand's main account are from the US. Second, the UK is a predomentaly English speaking and writing country, enabling me to process the text of the replies. Third, I could not find this same level of data publicly available for any other country satisfying the above two criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules used for analysis, version numbers are available in packages.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import dill\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection <a name=\"DataCollection\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the survey data and Twitter data were scraped using the Selenium library. The benifit of Selenium for this project comes from the ability to click on buttons, collect data not in a tabular form, and navigate to links within other webpages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survey Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data comes from https://yougov.co.uk/ratings/consumer/fame/brands/all (collected 2022 Q2 data). Only brands with fame of at least 95% were included. This criteria was chosen with the assumption that more famous brands are more likely to have regional twitter accounts, and with a greater number of replies than their less famous conterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_data_url = \"https://yougov.co.uk/ratings/consumer/fame/brands/all\" #survey data site\n",
    "\n",
    "def page_down_wait_one_second(webdriver, body='/html/body'):\n",
    "    webdriver.find_element(By.XPATH, body).send_keys(Keys.PAGE_DOWN)\n",
    "    time.sleep(1)\n",
    "\n",
    "def repeat_page_down(number_of_times, webdriver, body='/html/body'):\n",
    "    for x in range(0,number_of_times):\n",
    "        page_down_wait_one_second(webdriver, body='/html/body')\n",
    "\n",
    "# chunk below: going to survey url and getting all data on screen\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(public_data_url)\n",
    "driver.find_element(By.ID, \"onetrust-accept-btn-handler\").send_keys(Keys.ENTER)\n",
    "driver.find_element(By.NAME, \"rankings-load-more-entities\").send_keys(Keys.ENTER)\n",
    "repeat_page_down(70, driver) # after viewing the webpage, pressing page down 70 times is sufficient to capture all relevant data on screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get link for yougov profile for each brand. This page has more robust sentiment data than the main list\n",
    "all_brands = driver.find_elements(By.XPATH, '//yg-rankings-entities-list//a')\n",
    "all_brands_links = [brand.get_attribute(\"href\") for brand in all_brands]\n",
    "above_95 = all_brands_links[0:343] # only include brands with fame >= 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_driver = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each url and scrape sentiment data\n",
    "brands_sentiment = []\n",
    "for link in above_95:\n",
    "    temp_driver.get(link)\n",
    "    time.sleep(10) # Selenium risks recording incomplete data if there is not sufficient time for the page to load\n",
    "    just_sentiment = [item.text for item in temp_driver.find_elements(By.CLASS_NAME, \"value\")]\n",
    "    just_name = temp_driver.find_element(By.CLASS_NAME, \"entity-name\").text\n",
    "    brand_sentiment_and_name = [just_name, just_sentiment[0], just_sentiment[1], just_sentiment[2], just_sentiment[3]]\n",
    "    brands_sentiment.append(brand_sentiment_and_name)\n",
    "    print(brand_sentiment_and_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing percent sign and casting as float\n",
    "for brand in brands_sentiment:\n",
    "    brand[1] = float( str.replace(brand[1], '%', \"\") )\n",
    "    brand[2] = float( str.replace(brand[2], '%', \"\") )\n",
    "    brand[3] = float( str.replace(brand[3], '%', \"\") )\n",
    "    brand[4] = float( str.replace(brand[4], '%', \"\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving survey data to csv\n",
    "df_public = pd.DataFrame(brands_sentiment)\n",
    "df_public.columns = ['name', 'fame', 'positive', 'negative', 'neutral']\n",
    "df_public.to_csv('public_data.csv')\n",
    "driver.close()\n",
    "temp_driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Data\n",
    "\n",
    "I hand collected the twitter handles for companies with fame greater than 95%. This data is stored on the famous_brands.csv file.\n",
    "\n",
    "The inclusion criteria is as follows:\n",
    "- The brand's Twitter handle must contain the prefix or suffix 'UK'.\n",
    "- A subsidiary brand like Peanut M&M will have its larger brand included (M&M), unless there exists an account for the subsidiary brand.\n",
    "- If multiple accounts claiming to be the twitter account of a brand are shown in the search bar, and none of them are are verified, no account will be added."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "famous_brands_with_twitter_df = pd.read_csv('famous_brands_with_twitter.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying that each brand in famous_brands_with_twitter_df is in public_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_names_with_twitter = list(famous_brands_with_twitter_df['name'])\n",
    "public_brand_names = list(df_public['name'])\n",
    "\n",
    "not_included = []\n",
    "for brand in brand_names_with_twitter:\n",
    "    if brand.upper() not in public_brand_names:\n",
    "        not_included.append(brand)\n",
    "\n",
    "print(not_included)\n",
    "print(len(brand_names_with_twitter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Twitter handle variable to survey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining twitter handle to dataframe with survey sentiment data\n",
    "famous_brands_with_twitter_df['name'] = famous_brands_with_twitter_df['name'].apply(lambda x: str.upper(x))\n",
    "public_with_twitter_df = pd.merge(df_public, famous_brands_with_twitter_df, on='name')\n",
    "public_with_twitter_df.drop(columns='Unnamed: 0', inplace=True)\n",
    "public_with_twitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_with_twitter_df.to_csv('merged_corp_public.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_search_url = 'https://twitter.com/search-advanced?lang=en'\n",
    "\n",
    "def q2_search_url(handle):\n",
    "    return f'https://twitter.com/search?lang=en&q=(to%3A{handle})%20until%3A2022-06-30%20since%3A2022-04-01&src=typed_query'\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code does three things for each brand\n",
    "1. Search for tweets sent in reply to the brand during Q2 period\n",
    "2. Scrape all these tweets\n",
    "3. Create a dict with handle as key, with a list of all tweets to that brand as the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_handles = pd.read_csv('merged_corp_public.csv')\n",
    "twitter_handles = list(twitter_handles['twitter_uk'])\n",
    "\n",
    "brand_replies = {}\n",
    "for handle in twitter_handles:\n",
    "    driver.get(q2_search_url(handle))\n",
    "    time.sleep(10) # allow sufficient time for page to fully load\n",
    "    # After scrolling some distance down, earlier tweets will not be accessable from the HTML file.\n",
    "    # To account for this, all available tweets will be scraped after every page down, and duplicate tweets will be removed later\n",
    "    tweets_with_duplicates = []\n",
    "    for pg_down in range(50):\n",
    "        #scrape text from tweets\n",
    "        tweet_objects = driver.find_elements(By.XPATH, '//div[@data-testid=\"tweetText\"]')    \n",
    "        for tweet in tweet_objects:\n",
    "            tweets_with_duplicates.append(tweet.text)\n",
    "        repeat_page_down(1, driver)\n",
    "    # including only unique tweets\n",
    "    tweets = []\n",
    "    for tweet in tweets_with_duplicates:\n",
    "        if tweet not in tweets:\n",
    "            tweets.append(tweet)\n",
    "    brand_replies.update({handle: tweets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"brand_replies.json\", \"w\") as file:\n",
    "    json.dump(brand_replies, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning <a name=\"DataCleaning\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying every handle in the twitter replies corresponds with a handle in the merged_corp_public dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "bool_test = []\n",
    "for brand in brand_replies.keys():\n",
    "    bool_test.append(brand in list(twitter_handles))\n",
    "\n",
    "print(False in bool_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying correct data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name           object\n",
       "fame          float64\n",
       "positive      float64\n",
       "negative      float64\n",
       "neutral       float64\n",
       "twitter_uk     object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "public_with_twitter_df.dtypes #correct data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing Text to Remove Symbols and Non-English Letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('brand_replies.json') as file:\n",
    "    brand_replies = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = []\n",
    "number_of_characters = 0\n",
    "for brand in brand_replies:\n",
    "    for reply in brand_replies[brand]:\n",
    "        for char in reply:\n",
    "            symbols.append(char)\n",
    "            number_of_characters += 1\n",
    "symbols = set(symbols)\n",
    "\n",
    "english_letters_and_space = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', ' ']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_letters_and_space = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', ' ']\n",
    "\n",
    "# replace newline with space\n",
    "for brand in brand_replies:\n",
    "    replies_no_symbols = []\n",
    "    for reply in brand_replies[brand]:\n",
    "        reply = reply.replace('\\n', \"\")\n",
    "        replies_no_symbols.append(reply)\n",
    "    brand_replies.update({brand : replies_no_symbols})  \n",
    "\n",
    "# removing all non-english symbols from tweets, allowing for greater text processing\n",
    "for brand in brand_replies:\n",
    "    replies_no_symbols = []\n",
    "    for reply in brand_replies[brand]:\n",
    "        english_reply = reply # initializes vairable that will hold a reply containing only english letters\n",
    "        for symbol in symbols:\n",
    "            if symbol not in english_letters_and_space:\n",
    "                english_reply = english_reply.replace(symbol, \"\")\n",
    "        replies_no_symbols.append(english_reply)\n",
    "    brand_replies.update({brand : replies_no_symbols})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"brand_replies.json\", \"w\") as file:\n",
    "    json.dump(brand_replies, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Survey data has values within valid range of 0 and 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "num_of_fame_violations = len(public_with_twitter_df[ (public_with_twitter_df[\"fame\"] > 100) | (public_with_twitter_df[\"fame\"] < 0) ])\n",
    "num_of_positive_violations = len(public_with_twitter_df[ (public_with_twitter_df[\"positive\"] > 100) | (public_with_twitter_df[\"positive\"] < 0) ])\n",
    "num_of_negative_violations = len(public_with_twitter_df[ (public_with_twitter_df[\"negative\"] > 100) | (public_with_twitter_df[\"negative\"] < 0) ])\n",
    "num_of_neutral_violations = len(public_with_twitter_df[ (public_with_twitter_df[\"neutral\"] > 100) | (public_with_twitter_df[\"neutral\"] < 0) ])\n",
    "num_of_total_violations = num_of_fame_violations + num_of_positive_violations +num_of_negative_violations + num_of_neutral_violations\n",
    "print(num_of_total_violations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, Fame = Positive + Negative + Neutral. However as shown below, rounding/truncating causes the formula to occasionally be slightly off. Beyond setting the inclusion criteria, the fame variable plays no role in the remainder of the analysis. As such the violation poses no threat to analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "1.0\n",
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "print(len(public_with_twitter_df[ public_with_twitter_df['fame'] != (public_with_twitter_df[\"positive\"] + public_with_twitter_df[\"negative\"] + public_with_twitter_df[\"neutral\"]) ])) # many rows violate the fame = positive + negative + neutral\n",
    "# testing if discrepency is due to rounding\n",
    "rounding_test = pd.Series(public_with_twitter_df['fame'] - (public_with_twitter_df[\"positive\"] + public_with_twitter_df[\"negative\"] + public_with_twitter_df[\"neutral\"]))\n",
    "print(max(rounding_test))\n",
    "print(min(rounding_test))\n",
    "# since the difference between public_with_twitter_df['fame'] and (public_with_twitter_df[\"positive\"] + public_with_twitter_df[\"negative\"] + public_with_twitter_df[\"neutral\"]) is at most 1 point away, and the original data did not have\n",
    "# fractions, the difference is most likely due to rounding/truncating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "There were no missing values in the survey data. There were multiple brands that did not get responses to their tweets in Q2 2022. Since no sentiment analysis could be preformed on these brands, they were dropped from analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name          0\n",
       "fame          0\n",
       "positive      0\n",
       "negative      0\n",
       "neutral       0\n",
       "twitter_uk    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for survey data\n",
    "public_with_twitter_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tweets\n",
    "num_of_brands_no_replies = 0\n",
    "brands_no_replies = []\n",
    "for brand in brand_replies:\n",
    "    if brand_replies[brand] == []:\n",
    "        num_of_brands_no_replies += 1\n",
    "        brands_no_replies.append(brand)\n",
    "\n",
    "# As no twitter data exists to estimate sentiment distribution, these brands will be dropped\n",
    "for brand in brands_no_replies:\n",
    "    del brand_replies[brand]\n",
    "\n",
    "# brands_no_replies in public_with_twitter_df[\"twitter_uk\"]\n",
    "just_handles = public_with_twitter_df[\"twitter_uk\"]\n",
    "public_with_twitter_df = public_with_twitter_df[ public_with_twitter_df['twitter_uk'].apply(lambda handle: False if (handle in brands_no_replies) else True) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating additional Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 additional variables were created as defined below:\n",
    "1. like_50_survey = 1 if at least 50% of respondents had a positive impression of a brand, 0 otherwise\n",
    "2. like_50_prediction = 1 if replies to a brand was at least 50% positive, 0 otherwise\n",
    "3. twitter_more_popular = 1 if a brand had a greater percent of positive responses than positive impression from survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter_vs_public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8235294117647058"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['like_50_survey'] = df['positive_survey'].apply(lambda x: 1 if (x >= 50) else 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27450980392156865"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['like_50_prediction'] = df['positive_prediction'].apply(lambda x: 1 if (x >= .50) else 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11764705882352941"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['twitter_more_positive'] = (df['positive_prediction']*100 > df['positive_survey']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('twitter_vs_public.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out Brands with less than 10 Replies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding Brands with less than 10 replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('brand_replies.json') as file:\n",
    "    brand_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_handles , brand_replies = list(brand_dict.keys()), list(brand_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_counter_per_brand = []\n",
    "for brand in brand_replies:\n",
    "    counter = 0\n",
    "    for tweet in brand:\n",
    "        counter += 1\n",
    "    reply_counter_per_brand.append(counter)\n",
    "\n",
    "reply_counter_per_brand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining twitter reply count to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter_vs_public.csv')\n",
    "df_to_merge = pd.DataFrame(list(zip(brand_handles,reply_counter_per_brand)), columns=['twitter_uk', 'reply_counter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df_to_merge, on='twitter_uk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "102\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "print(len(df[ df['reply_counter'] >= 10 ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[ df['reply_counter'] >= 10 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('twitter_vs_public.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis <a name=\"Analysis\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flair is a pretrained neural network used for sentiment classification. As demonstrated below, Flair has a high accuracy for predicting the sentiment of amazon reviews. One star reviews were used for negative sentiment, five star reviews were used for positive sentiment. 10000 reviews from each category were randomly chosen and passed through the model for classification. The model demonstrated an accuracy and precision greater than 90%.\n",
    "\n",
    "Flair classifies data as either 'Positive' or 'Negative'. However, some text may be neutral. This is examined in greater detail in the appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon and Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment:\n",
    "    negative = 'NEGATIVE'\n",
    "    positive = 'POSITIVE'\n",
    "    neutral = 'NEUTRAL'\n",
    "\n",
    "class Review:\n",
    "    def __init__(self, text, rating):\n",
    "        self.text = text\n",
    "        self.rating = int(rating)\n",
    "        self.sentiment = self.get_sentiment()\n",
    "    \n",
    "    def get_sentiment(self):\n",
    "        if self.rating == 5:\n",
    "            return Sentiment.positive\n",
    "        elif self.rating == 1:\n",
    "            return Sentiment.negative\n",
    "        elif self.rating == 3:\n",
    "            return Sentiment.neutral\n",
    "        else: return \"DROP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data\n",
    "amazon_data = []\n",
    "with open('kcore_5.json') as file:\n",
    "    num = 0\n",
    "    for line in file:\n",
    "        review = json.loads(line)\n",
    "        amazon_data.append(Review(review[\"reviewText\"], review[\"overall\"]))\n",
    "        num += 1\n",
    "        if num >= 250000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of 1 star reviews:  12706\n",
      "number of 5 star reviews:  142122\n"
     ]
    }
   ],
   "source": [
    "one_star_reviews = []\n",
    "five_star_reviews = []\n",
    "for review in amazon_data:\n",
    "    if review.text == \"\":\n",
    "        amazon_data.remove(review)\n",
    "        continue\n",
    "    if review.rating == 1:\n",
    "        one_star_reviews.append(review)\n",
    "    elif review.rating == 5:\n",
    "        five_star_reviews.append(review)\n",
    "\n",
    "print(\"number of 1 star reviews: \",len(one_star_reviews))\n",
    "print(\"number of 5 star reviews: \",len(five_star_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#selecting a subset of data for testing flair model\n",
    "random.seed(0)\n",
    "ten_k_one_star = random.sample(one_star_reviews,10000)\n",
    "ten_k_five_star = random.sample(five_star_reviews,10000)\n",
    "aggregated_samples = []\n",
    "for sample in [ten_k_one_star, ten_k_five_star]:\n",
    "    for draw in sample:\n",
    "        aggregated_samples.append(draw)\n",
    "\n",
    "len(\"number of observations: \",aggregated_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_reviews = []\n",
    "for review in aggregated_samples:\n",
    "    text_reviews.append(review.text)\n",
    "\n",
    "numerical_reviews = []\n",
    "for review in aggregated_samples:\n",
    "    numerical_reviews.append(review.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-16 16:28:04,670 loading file C:\\Users\\franc\\.flair\\models\\sentiment-en-mix-distillbert_4.pt\n"
     ]
    }
   ],
   "source": [
    "classifier = TextClassifier.load('en-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sentiment = []\n",
    "\n",
    "for review in text_reviews:\n",
    "    sentence = Sentence(review)\n",
    "    classifier.predict(sentence)\n",
    "\n",
    "    # taking prediction from object form into string\n",
    "    temp_string = str.split(sentence.labels[0].__str__(), '→')[1]\n",
    "    temp_string = str.split(temp_string,'(')[0].strip()\n",
    "\n",
    "\n",
    "    predicted_sentiment.append(temp_string) #POSITIVE and NEGATIVE options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_sentiment_data = pd.DataFrame(list(zip(text_reviews, numerical_reviews, predicted_sentiment)), columns=[\"text_reviews\", \"true_stars_reviews\", \"predicted_sentiment\"])\n",
    "    \n",
    "\n",
    "def stars_to_words(series):\n",
    "    if series == 1:\n",
    "        return 'NEGATIVE'\n",
    "    if series == 3:\n",
    "        return 'NEUTRAL'\n",
    "    if series == 5:\n",
    "        return 'POSITIVE'\n",
    "\n",
    "\n",
    "amazon_sentiment_data['true_sentiment_reviews'] = amazon_sentiment_data['true_stars_reviews'].apply(lambda x: stars_to_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9477\n",
      "0.9690905280804694\n",
      "0.9281752104055088\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Accuracy Score: \",accuracy_score(amazon_sentiment_data[\"true_sentiment_reviews\"], amazon_sentiment_data[\"predicted_sentiment\"]))\n",
    "print(\"Accuracy Score: \",precision_score(amazon_sentiment_data[\"true_sentiment_reviews\"], amazon_sentiment_data[\"predicted_sentiment\"], pos_label=Sentiment.positive))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Twitter Reply Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code transforms the twitter data to have one response on each row, rather than one row containing every reply to a brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('brand_replies.json') as file:\n",
    "    brand_replies = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_list = []\n",
    "reply_list = []\n",
    "sentiment_list = []\n",
    "\n",
    "for brand in brand_replies:\n",
    "    for reply in brand_replies[brand]:\n",
    "        brand_list.append(brand)\n",
    "        reply_list.append(reply)\n",
    "        \n",
    "\n",
    "flat_reply = pd.DataFrame(list(zip(brand_list, reply_list)), columns=['brand', 'reply'])\n",
    "print(len(flat_reply))\n",
    "flat_reply = flat_reply[ flat_reply['reply'] != \"\"]\n",
    "flat_reply = flat_reply[ flat_reply['reply'] != \" \"]\n",
    "print(len(flat_reply))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis, Twitter\n",
    "\n",
    "applying the Flair model to scraped tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sentiment = []\n",
    "\n",
    "for reply in flat_reply['reply']:\n",
    "    sentence = Sentence(reply)\n",
    "    classifier.predict(sentence)\n",
    "    # taking prediction from object form into string\n",
    "    temp_string = str.split(sentence.labels[0].__str__(), '→')[1]\n",
    "    temp_string = str.split(temp_string,'(')[0].strip()\n",
    "\n",
    "    predicted_sentiment.append(temp_string) #POSITIVE and NEGATIVE options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sentiment_dummy = []\n",
    "positive_prediction = []\n",
    "negative_prediction = []\n",
    "\n",
    "for prediction in predicted_sentiment:\n",
    "    if prediction == Sentiment.positive:\n",
    "        predicted_sentiment_dummy.append(1)\n",
    "        positive_prediction.append(1)\n",
    "        negative_prediction.append(0)\n",
    "    if prediction == Sentiment.negative:\n",
    "        predicted_sentiment_dummy.append(0)\n",
    "        negative_prediction.append(1)\n",
    "        positive_prediction.append(0)\n",
    "\n",
    "#flat_reply[\"classification\"] = predicted_sentiment_dummy\n",
    "flat_reply[\"positive_prediction\"] = positive_prediction\n",
    "flat_reply[\"negative_prediction\"] = negative_prediction\n",
    "flat_reply[\"total_prediction\"] = flat_reply['positive_prediction'] + flat_reply['negative_prediction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating predicted sentiment data to merge with survey data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_twitter = flat_reply.groupby('brand').mean(['positive_prediction', 'negative_prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_twitter.to_csv('brand_classification.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging survey data and classification summary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_twitter = pd.read_csv('brand_classification.csv')\n",
    "merged_corp_public_df = pd.read_csv('merged_corp_public.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "124\n",
      "116\n"
     ]
    }
   ],
   "source": [
    "#creating shared column name\n",
    "\n",
    "grouped_twitter.rename(columns={'brand': 'twitter_uk'}, inplace=True)\n",
    "\n",
    "print(len(grouped_twitter))\n",
    "print(len(merged_corp_public_df))\n",
    "df = pd.merge(merged_corp_public_df, grouped_twitter, on='twitter_uk', how='inner')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error in data collection, Tescos and Tesco Express were both added when they shared the same twitter handle. Tesco Express is dropped below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[ df['twitter_uk'] == 'TescosUK' ]\n",
    "df = df[ df['name'] != 'TESCO EXPRESS' ] # 1 obs was removed, error in data collection, they share the same twitter handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['fame', 'neutral', 'Unnamed: 0'], inplace=True) #not necessary for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'positive': 'positive_survey', 'negative': 'negative_survey'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('twitter_vs_public.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results <a name=\"Results\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%html\n",
    "<div class='tableauPlaceholder' id='viz1661448710888' style='position: relative'><noscript><a href='#'><img alt='Dashboard 1 ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Tw&#47;TwittervsPublic&#47;Dashboard1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='TwittervsPublic&#47;Dashboard1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Tw&#47;TwittervsPublic&#47;Dashboard1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='language' value='en-US' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1661448710888');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='1526px';vizElement.style.height='878px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix <a name=\"Appendix\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentiment classification model only classifies a string as positive or negative. As such tweets that were neutral in sentiment were classified as positive or negative. This section aims to estimates the classification distribution of neutral text via amazon review data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing data\n",
    "amazon_data = []\n",
    "with open('kcore_5.json') as file:\n",
    "    num = 0\n",
    "    for line in file:\n",
    "        review = json.loads(line)\n",
    "        amazon_data.append(Review(review[\"reviewText\"], review[\"overall\"]))\n",
    "        num += 1\n",
    "        if num >= 250000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of 3 star reviews:  26019\n"
     ]
    }
   ],
   "source": [
    "three_star_reviews = []\n",
    "for review in amazon_data:\n",
    "    if review.text == \"\":\n",
    "        amazon_data.remove(review)\n",
    "        continue\n",
    "    if review.rating == 3:\n",
    "        three_star_reviews.append(review)\n",
    "\n",
    "\n",
    "print(\"number of 3 star reviews: \",len(three_star_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#selecting a subset of data for testing flair model\n",
    "random.seed(0)\n",
    "ten_k_three_star = random.sample(three_star_reviews,10000)\n",
    "aggregated_samples = []\n",
    "for sample in [ten_k_three_star]:\n",
    "    for draw in sample:\n",
    "        aggregated_samples.append(draw)\n",
    "\n",
    "len(aggregated_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_reviews = []\n",
    "for review in aggregated_samples:\n",
    "    text_reviews.append(review.text)\n",
    "\n",
    "numerical_reviews = []\n",
    "for review in aggregated_samples:\n",
    "    numerical_reviews.append(review.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-17 08:52:14,279 loading file C:\\Users\\franc\\.flair\\models\\sentiment-en-mix-distillbert_4.pt\n"
     ]
    }
   ],
   "source": [
    "classifier = TextClassifier.load('en-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sentiment = []\n",
    "\n",
    "for review in text_reviews:\n",
    "    sentence = Sentence(review)\n",
    "    classifier.predict(sentence)\n",
    "\n",
    "    # taking prediction from object form into string\n",
    "    temp_string = str.split(sentence.labels[0].__str__(), '→')[1]\n",
    "    temp_string = str.split(temp_string,'(')[0].strip()\n",
    "\n",
    "\n",
    "    predicted_sentiment.append(temp_string) #POSITIVE and NEGATIVE options\n",
    "\n",
    "predicted_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_sentiment_data = pd.DataFrame(list(zip(text_reviews, numerical_reviews, predicted_sentiment)), columns=[\"text_reviews\", \"true_stars_reviews\", \"predicted_sentiment\"])\n",
    "    \n",
    "\n",
    "def stars_to_words(series):\n",
    "    if series == 1:\n",
    "        return 'NEGATIVE'\n",
    "    if series == 3:\n",
    "        return 'NEUTRAL'\n",
    "    if series == 5:\n",
    "        return 'POSITIVE'\n",
    "\n",
    "\n",
    "amazon_sentiment_data['true_sentiment_reviews'] = amazon_sentiment_data['true_stars_reviews'].apply(lambda x: stars_to_words(x))\n",
    "amazon_sentiment_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the 3 star reviews, 74% of them were assigned as negative and 26% were assigned positive. From personal experience and opinion, 3 star amazon reviews tend to skew negative, which may partially account for the difference, but is nevertheless important to keep in mind when considering this analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of 3 star reviews predicted positive are:  0.2578\n",
      "the number of 3 star reviews predicted negative are:  0.7422\n"
     ]
    }
   ],
   "source": [
    "only_threes = amazon_sentiment_data[ amazon_sentiment_data['true_stars_reviews'] == 3 ]\n",
    "print('the number of 3 star reviews predicted positive are: ' ,len( only_threes[ only_threes['predicted_sentiment'] == Sentiment.positive ])/len(only_threes))\n",
    "print('the number of 3 star reviews predicted negative are: ' ,len( only_threes[ only_threes['predicted_sentiment'] == Sentiment.negative ])/len(only_threes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 ('public_vs_twitter_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "203103cac922ad4d21d6ac9a664b6fce2c05f2559d9918e7761fb6ac03e36d01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
